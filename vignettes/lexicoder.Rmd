---
title: "Proximity-weighted text analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{proximity-weighted text analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
suppressPackageStartupMessages(library(quanteda))
library(quanteda.proximity)
```

`data_corpus_inaugural` is a built-in dataset of `quanteda`. This is the usual way to conduct a dictionary-based analysis using the Lexicoder dictionary (also built-in). The "Net Tone" measure is calculated (Young & Soroka, 2012).

```{r inaugural}
data_corpus_inaugural %>% tokens %>% tokens_tolower %>% tokens_compound(data_dictionary_LSD2015, concatenator = " ") -> tok1

tok1 %>% dfm %>% dfm_lookup(data_dictionary_LSD2015) %>%
        convert("data.frame") -> dat1
data_corpus_inaugural %>% tokens %>% tokens_tolower %>% tokens_compound(data_dictionary_LSD2015, concatenator = " ") %>%
    ntoken() -> token_count

dat1$total_count <- token_count

dat1$nettone <- ((dat1$positive + dat1$neg_negative) / dat1$total_count) -
    ((dat1$negative + dat1$neg_positive) / dat1$total_count)
dat1[order(dat1$nettone, decreasing = TRUE),c("doc_id", "nettone")]
```

The Net Tone measures the overall tone of the entire article. That might not be very interesting, because we don't know what words exactly are associated with the so-called positive and negative tones.

## Alternative I - Window

An alternative is to consider words that are within a certain window of some keywords you are interested in. It can be done purely with `quanteda`. Suppose you are interested in all "ameri*" keywords and a window of 20.

```{r window}
tok1 %>% tokens_select(c("ameri*"), window = 20) %>% dfm %>% dfm_lookup(data_dictionary_LSD2015) %>%
        convert("data.frame") -> dat2
dat2$total_count <- token_count
dat2$nettone <- ((dat2$positive + dat2$neg_negative) / dat2$total_count) -
    ((dat2$negative + dat2$neg_positive) / dat2$total_count)
dat2[order(dat2$nettone, decreasing = TRUE),c("doc_id", "nettone")]
```

## Alternative II - Proximity

We can weight the frequency by the proximity to "ameri*" keywords.

```{r}
tok1 %>% tokens_proximity(c("ameri*")) %>% dfm %>% dfm_lookup(data_dictionary_LSD2015) %>%
        convert("data.frame") -> dat3
dat3$total_count <- token_count
dat3$nettone <- ((dat3$positive + dat3$neg_negative) / dat3$total_count) -
    ((dat3$negative + dat3$neg_positive) / dat3$total_count)
dat3[order(dat3$nettone, decreasing = TRUE),c("doc_id", "nettone")]
```
